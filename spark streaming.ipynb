{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming application using Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write code to create a SparkSession, which uses four cores with a proper application name, use the Melbourne timezone, and make sure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "check_point = \"/Users/sanchita/Documents/SEM2/BigData/FIT5202/Assignment3\"\n",
    "melbourne_timezone = \"Australia/Melbourne\"\n",
    "#configuring the spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MOTH Predictive Model and Analysis\") \\\n",
    "    .config(\"spark.master\", \"local[4]\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", melbourne_timezone) \\\n",
    "    .config(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\")\\\n",
    "    .config(\"spark.checkpoint.dir\", \"stream_parquet/click_stream\")\\\n",
    "    .config(\"spark.checkpoint.dir\", \"sales_parquet/revenue\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. customer, product, category) into data frames. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages to define schemas for the datasets based on the metadata\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    "    DateType,\n",
    ")\n",
    "\n",
    "# defining the schema for category schema\n",
    "category_schema = StructType(\n",
    "    [\n",
    "        StructField(\"index\", IntegerType(), False),\n",
    "        StructField(\"category_id\", IntegerType(), False),\n",
    "        StructField(\"cat_level1\", StringType(), True),\n",
    "        StructField(\"cat_level2\", StringType(), True),\n",
    "        StructField(\"cat_level3\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# defining for customer schema\n",
    "customer_schema = StructType(\n",
    "    [\n",
    "        StructField(\"index\", IntegerType(), False),\n",
    "        StructField(\"customer_id\", IntegerType(), False),\n",
    "        StructField(\"first_name\", StringType(), True),\n",
    "        StructField(\"last_name\", StringType(), True),\n",
    "        StructField(\"username\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"birthdate\", DateType(), True),\n",
    "        StructField(\"device_type\", StringType(), True),\n",
    "        StructField(\"device_id\", StringType(), True),\n",
    "        StructField(\"device_version\", StringType(), True),\n",
    "        StructField(\"home_location_lat\", DoubleType(), True),\n",
    "        StructField(\"home_location_long\", DoubleType(), True),\n",
    "        StructField(\"home_location\", StringType(), True),\n",
    "        StructField(\"home_country\", StringType(), True),\n",
    "        StructField(\"first_join_date\", DateType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# defining the product schema\n",
    "product_schema = StructType(\n",
    "    [\n",
    "        StructField(\"index\", IntegerType(), False),\n",
    "        StructField(\"product_id\", IntegerType(), False),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"baseColour\", StringType(), True),\n",
    "        StructField(\"season\", StringType(), True),\n",
    "        StructField(\"year\", IntegerType(), True),\n",
    "        StructField(\"usage\", StringType(), True),\n",
    "        StructField(\"productDisplayName\", StringType(), True),\n",
    "        StructField(\"category_id\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# defining clisk_stream schema\n",
    "click_schema = StructType(\n",
    "    [\n",
    "        StructField(\"index\", IntegerType(), False),\n",
    "        StructField(\"session_id\", StringType(), False),\n",
    "        StructField(\"event_name\", StringType(), True),\n",
    "        StructField(\"event_time\", TimestampType(), True),\n",
    "        StructField(\"event_id\", StringType(), True),\n",
    "        StructField(\"traffic_source\", StringType(), True),\n",
    "        StructField(\"event_metadata\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# defining the transaction schema\n",
    "transaction_schema = StructType(\n",
    "    [\n",
    "        StructField(\"index\", IntegerType(), False),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"booking_id\", StringType(), False),\n",
    "        StructField(\"session_id\", StringType(), True),\n",
    "        StructField(\"product_metadata\", StringType(), True),\n",
    "        StructField(\"payment_method\", StringType(), True),\n",
    "        StructField(\"payment_status\", StringType(), True),\n",
    "        StructField(\"promo_amount\", DoubleType(), True),\n",
    "        StructField(\"promo_code\", StringType(), True),\n",
    "        StructField(\"shipment_fee\", DoubleType(), True),\n",
    "        StructField(\"shipment_date_limit\", DateType(), True),\n",
    "        StructField(\"shipment_location_lat\", DoubleType(), True),\n",
    "        StructField(\"shipment_location_long\", DoubleType(), True),\n",
    "        StructField(\"total_amount\", DoubleType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Schema: \n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- cat_level1: string (nullable = true)\n",
      " |-- cat_level2: string (nullable = true)\n",
      " |-- cat_level3: string (nullable = true)\n",
      "\n",
      "Customer Schema: \n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_version: string (nullable = true)\n",
      " |-- home_location_lat: double (nullable = true)\n",
      " |-- home_location_long: double (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- home_country: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      "\n",
      "Product Schema: \n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- baseColour: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- productDisplayName: string (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      "\n",
      "Click Stream Schema: \n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      "\n",
      "Transaction Schema: \n",
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- booking_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: double (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- shipment_fee: double (nullable = true)\n",
      " |-- shipment_date_limit: date (nullable = true)\n",
      " |-- shipment_location_lat: double (nullable = true)\n",
      " |-- shipment_location_long: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "Customer Session Schema: \n",
      "root\n",
      " |-- #: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loading customer data into a dataframe with the customer schema\n",
    "# defining the parts for each file\n",
    "category_path = \"a2a_dataset/category.csv\"\n",
    "customer_path = \"a2a_dataset/customer.csv\"\n",
    "product_path = \"a2a_dataset/product.csv\"\n",
    "cust_session_path = \"a2a_dataset/customer_session.csv\"\n",
    "clickstream_path = \"click_stream.csv\"\n",
    "transaction_path = \"new_transactions.csv\"\n",
    "\n",
    "# reading the files based on the above schema and loading them into seperate dataframes\n",
    "category_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(category_schema)\n",
    "    .load(category_path)\n",
    ")\n",
    "customer_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customer_schema)\n",
    "    .load(customer_path)\n",
    ")\n",
    "product_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(product_schema)\n",
    "    .load(product_path)\n",
    ")\n",
    "clickstream_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(click_schema)\n",
    "    .load(clickstream_path)\n",
    ")\n",
    "transaction_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(transaction_schema)\n",
    "    .load(transaction_path)\n",
    ")\n",
    "cust_session_df = (\n",
    "    spark.read.format(\"csv\").option(\"header\", \"true\").load(cust_session_path)\n",
    ")\n",
    "\n",
    "print(\"Category Schema: \")\n",
    "category_df.printSchema()\n",
    "print(\"Customer Schema: \")\n",
    "customer_df.printSchema()\n",
    "print(\"Product Schema: \")\n",
    "product_df.printSchema()\n",
    "print(\"Click Stream Schema: \")\n",
    "clickstream_df.printSchema()\n",
    "print(\"Transaction Schema: \")\n",
    "transaction_df.printSchema()\n",
    "print(\"Customer Session Schema: \")\n",
    "cust_session_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Using the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "\n",
    "#reading data from kafka topic\n",
    "#defining the hostip\n",
    "hostip = hostip = \"192.168.0.250\" #change the IP4 address as needed\n",
    "\n",
    "topic = \"customer_clickstream_topic\"\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "  .option(\"subscribe\", topic) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read data from Kafka\n",
    "kafka_df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "\n",
    "#defining schema for the message from Kafka producer\n",
    "kafka_schema = StructType([\n",
    "    StructField(\"click_stream_data\", ArrayType(StructType([\n",
    "        StructField(\"session_id\", StringType(), True),\n",
    "        StructField(\"event_name\", StringType(), True),\n",
    "        StructField(\"event_id\", StringType(), True),\n",
    "        StructField(\"traffic_source\", StringType(), True),\n",
    "        StructField(\"event_metadata\", StringType(), True),\n",
    "        StructField(\"customer_id\", StringType(), True),\n",
    "        StructField(\"ts\", IntegerType(), True)\n",
    "    ]))),\n",
    "    StructField(\"sent_timestamp\", IntegerType())  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- click_stream_data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- session_id: string (nullable = true)\n",
      " |    |    |-- event_name: string (nullable = true)\n",
      " |    |    |-- event_id: string (nullable = true)\n",
      " |    |    |-- traffic_source: string (nullable = true)\n",
      " |    |    |-- event_metadata: string (nullable = true)\n",
      " |    |    |-- customer_id: string (nullable = true)\n",
      " |    |    |-- ts: integer (nullable = true)\n",
      " |-- sent_timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "#parsing the string\n",
    "parsed_kafka_df = kafka_df.select(F.from_json(F.col(\"value\").cast(\"string\"), kafka_schema).alias(\"data\")).select(\"data.*\")\n",
    "parsed_kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sent_timestamp: integer (nullable = true)\n",
      " |-- cust_clickstream: struct (nullable = true)\n",
      " |    |-- session_id: string (nullable = true)\n",
      " |    |-- event_name: string (nullable = true)\n",
      " |    |-- event_id: string (nullable = true)\n",
      " |    |-- traffic_source: string (nullable = true)\n",
      " |    |-- event_metadata: string (nullable = true)\n",
      " |    |-- customer_id: string (nullable = true)\n",
      " |    |-- ts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#exploding the data to get it in a usable formatS\n",
    "final_df = parsed_kafka_df.selectExpr(\"sent_timestamp\", \"explode(click_stream_data) as cust_clickstream\")\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = final_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sent_timestamp: integer (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- ts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#second level of explode\n",
    "final_click_df = final_df.select(\n",
    "    \"sent_timestamp\",\n",
    "    \"cust_clickstream.session_id\",\n",
    "    \"cust_clickstream.event_name\",\n",
    "    \"cust_clickstream.event_id\",\n",
    "    \"cust_clickstream.traffic_source\",\n",
    "    \"cust_clickstream.event_metadata\",\n",
    "    \"cust_clickstream.customer_id\",\n",
    "    \"cust_clickstream.ts\"\n",
    ")\n",
    "\n",
    "final_click_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = final_click_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Then, the streaming data format should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.  \n",
    "Perform the following tasks:  \n",
    "a) For the 'ts' column, convert it to the timestamp format, we will use it as event_time.  \n",
    "b) If the data is late for more than 1 minute, discard it.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sent_timestamp: integer (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime, unix_timestamp, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#converting the ts column to timestamp type and renaming it as event_time\n",
    "final_click_df = final_click_df.withColumn(\"event_time\", from_unixtime(col(\"ts\")).cast(\"timestamp\"))\n",
    "final_click_df = final_click_df.drop(\"ts\")\n",
    "final_click_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = final_click_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# #defining the one minute interval\n",
    "one_min_interval = current_timestamp() - expr(\"INTERVAL 1 MINUTE\")\n",
    "#defining watermark\n",
    "watermark_interval = \"1 minute\"\n",
    "#filtering the dataframe to disgard records more than 1 minute\n",
    "watermark_df = final_click_df.withWatermark(\"event_time\", watermark_interval)\n",
    "#disgarding late data\n",
    "filtered_df = watermark_df.filter(col(\"event_time\") > one_min_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = filtered_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5  \n",
    "Aggregate the streaming data frame by session id and create features you used in your assignment 2A model. (note: customer ID has already been included in the stream.)   \n",
    "Then, join the static data frames with the streaming data frame as our final data for prediction.  \n",
    "Perform data type/column conversion according to your ML model, and print out the Schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----------+--------------------+--------------------+--------------------+--------------+--------------+------------+-------------+------------+-------------------+---------------------+----------------------+------------+---------------------+-----------------+\n",
      "|index|          created_at|customer_id|          booking_id|          session_id|    product_metadata|payment_method|payment_status|promo_amount|   promo_code|shipment_fee|shipment_date_limit|shipment_location_lat|shipment_location_long|total_amount|total_amount_quartile|shipment_fee_paid|\n",
      "+-----+--------------------+-----------+--------------------+--------------------+--------------------+--------------+--------------+------------+-------------+------------+-------------------+---------------------+----------------------+------------+---------------------+-----------------+\n",
      "|    1|2022-06-17 21:20:...|       5898|346f74ca-a933-463...|4d51b69d-544f-44b...|[{'product_id': 1...|   Credit Card|       Success|         0.0|         null|         0.0|         2022-06-21|    -7.90625951111112|      110.632690530369|    186282.0|                   Q1|                0|\n",
      "|    2|2019-03-24 09:16:...|      14159|511f59f8-3ef5-438...|4d55839d-4b3e-406...|[{'product_id': 4...|   Credit Card|       Success|      4735.0|WEEKENDMANTAP|     10000.0|         2019-03-27|    -4.26351275671241|      105.489401701251|    271777.0|                   Q2|                1|\n",
      "|    3|2020-07-18 17:54:...|      22576|8e509f58-7f8d-421...|4d5839f4-313f-45c...|[{'product_id': 2...|         Gopay|       Success|         0.0|         null|     10000.0|         2020-07-21|    -7.91707661186231|       110.13187555325|    323489.0|                   Q3|                1|\n",
      "|    4|2021-08-05 16:17:...|      18696|29d32f23-a07a-4f2...|4d5ad667-a524-4de...|[{'product_id': 2...|         Gopay|       Success|         0.0|         null|     50000.0|         2021-08-08|    -7.39661418330981|      109.511262594032|    305028.0|                   Q3|                1|\n",
      "|    5|2018-08-16 08:46:...|      90136|a3e90650-4db3-408...|4d5e5a02-dfed-40e...|[{'product_id': 2...|   Credit Card|       Success|         0.0|         null|     10000.0|         2018-08-19|   -0.637290541399757|      109.492521253314|    853787.0|                   Q4|                1|\n",
      "+-----+--------------------+-----------+--------------------+--------------------+--------------------+--------------+--------------+------------+-------------+------------+-------------------+---------------------+----------------------+------------+---------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#making transformations on transaction_df\n",
    "from pyspark.sql.functions import expr, col, when\n",
    "\n",
    "#total_amount_quartile\n",
    "# calculating quantiles\n",
    "quartiles = transaction_df.approxQuantile(\"total_amount\", [0.25, 0.5, 0.75], 0.01)\n",
    "\n",
    "# defining conditions for the quantiles\n",
    "quartile_seg = [\n",
    "    col(\"total_amount\") <= quartiles[0],\n",
    "    (col(\"total_amount\") > quartiles[0]) & (col(\"total_amount\") <= quartiles[1]),\n",
    "    (col(\"total_amount\") > quartiles[1]) & (col(\"total_amount\") <= quartiles[2]),\n",
    "    (col(\"total_amount\") > quartiles[2])\n",
    "]\n",
    "\n",
    "# setting labels for the quartiles segregated above\n",
    "labels = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "#adding the qunatiles to the dataframe\n",
    "transaction_df = transaction_df.withColumn(\n",
    "    \"total_amount_quartile\",\n",
    "    when(quartile_seg[0], labels[0])\n",
    "    .when(quartile_seg[1], labels[1])\n",
    "    .when(quartile_seg[2], labels[2])\n",
    "    .otherwise(labels[3])\n",
    ")\n",
    "\n",
    "#adding shipment fee to the dataframe\n",
    "transaction_df = transaction_df.withColumn(\"shipment_fee_paid\", when(col(\"shipment_fee\") > 0, 1).otherwise(0))\n",
    "    \n",
    "transaction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+-----------+--------------------+--------------------+------+----------+-----------+--------------------+--------------------+-----------------+------------------+-------------------+------------+---------------+---+--------------+---------+\n",
      "|index|customer_id|first_name|  last_name|            username|               email|gender| birthdate|device_type|           device_id|      device_version|home_location_lat|home_location_long|      home_location|home_country|first_join_date|age|customer_since|city_tier|\n",
      "+-----+-----------+----------+-----------+--------------------+--------------------+------+----------+-----------+--------------------+--------------------+-----------------+------------------+-------------------+------------+---------------+---+--------------+---------+\n",
      "|    1|       2870|      Lala|    Maryati|671a0865-ac4e-4dc...|671a0865_ac4e_4dc...|     F|1996-06-14|        iOS|c9c0de76-0a6c-4ac...|iPhone; CPU iPhon...|-1.04334537209108|  101.360522769637|     Sumatera Barat|   Indonesia|     2019-07-21| 27|          1550|   tier 3|\n",
      "|    2|       8193|  Maimunah| Laksmiwati|83be2ba7-8133-48a...|83be2ba7_8133_48a...|     F|1993-08-16|    Android|fb331c3d-f42e-40f...|       Android 2.2.1|-6.21248901687818|  106.818849854085|       Jakarta Raya|   Indonesia|     2017-07-16| 30|          2285|   tier 1|\n",
      "|    3|       7279|   Bakiman|Simanjuntak|3250e5a3-1d23-467...|3250e5a3_1d23_467...|     M|1989-01-23|        iOS|d13dde0a-6ae1-43c...|iPad; CPU iPad OS...|-8.63160723349251|   116.42843637819|Nusa Tenggara Barat|   Indonesia|     2020-08-23| 34|          1151|   tier 3|\n",
      "|    4|      88813|   Cahyadi|  Maheswara|df797edf-b465-4a8...|df797edf_b465_4a8...|     M|1991-01-05|        iOS|f4c18515-c5be-419...|iPad; CPU iPad OS...| 1.29933154841027|  115.774933596524|   Kalimantan Timur|   Indonesia|     2021-10-03| 32|           745|   tier 1|\n",
      "|    5|      82542|   Irnanto|     Wijaya|36ab08e1-03de-42a...|36ab08e1_03de_42a...|     M|2000-07-15|        iOS|e46e4c36-4630-473...|iPhone; CPU iPhon...|-2.98080666922999|  114.924674565408| Kalimantan Selatan|   Indonesia|     2021-04-11| 23|           920|   tier 3|\n",
      "+-----+-----------+----------+-----------+--------------------+--------------------+------+----------+-----------+--------------------+--------------------+-----------------+------------------+-------------------+------------+---------------+---+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#making necessary transformation is customer_df\n",
    "from pyspark.sql.functions import datediff, current_date, round\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# calculating age in customer_df\n",
    "customer_df = customer_df.withColumn(\n",
    "    \"age\",\n",
    "    (datediff(current_date(), customer_df[\"birthdate\"]) / 365.25).cast(\n",
    "        \"integer\"\n",
    "    ),  # calculating in years\n",
    ")\n",
    "\n",
    "# calculating the period that the customer has been on the platform\n",
    "# defining a function to calculate the period of customer on platform\n",
    "def days_since_join(first_join_date):\n",
    "    current_date = F.current_date()\n",
    "    customer_since = F.datediff(current_date, first_join_date)\n",
    "    return round(customer_since)\n",
    "\n",
    "\n",
    "# Adding it to the feature_df\n",
    "customer_df = customer_df.withColumn(\n",
    "    \"customer_since\", days_since_join(F.col(\"first_join_date\")).cast(IntegerType())\n",
    ")\n",
    "\n",
    "# transformation of home_location to city-tiering in Indonesia for easy categorisation\n",
    "# defining location tiers in Indonation\n",
    "tier1_city = [\"Jakarta Raya\", \"Kalimantan Timur\"]  # Highest GDP and metropolitan\n",
    "tier2_city = [\n",
    "    \"Sulawesi Selatan\",\n",
    "    \"Jawa Barat\",\n",
    "    \"Riau\",\n",
    "    \"Jawa Timur\",\n",
    "    \"Jawa Tengah\",\n",
    "]  # Growing GDP and medium economy\n",
    "\n",
    "# creating a new column\n",
    "customer_df = customer_df.withColumn(\n",
    "    \"city_tier\",\n",
    "    when(col(\"home_location\").isin(tier1_city), \"tier 1\")\n",
    "    .when(col(\"home_location\").isin(tier1_city), \"tier 2\")\n",
    "    .otherwise(\"tier 3\"),\n",
    ")\n",
    "\n",
    "customer_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, sum\n",
    "\n",
    "# adding a feature to identify if a customer has used a promotion or not\n",
    "filtered_df = filtered_df.withColumn(\"is_promotion\", when(col(\"event_name\") == \"ADD PROMO\", 1)\n",
    "                                    .otherwise(0))\n",
    "\n",
    "#identifying the month of shopping by the customer\n",
    "filtered_df = filtered_df.withColumn(\"session_month\", F.month(filtered_df[\"event_time\"]))\n",
    "\n",
    "# assigning season based on month\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"session_season\",\n",
    "    when((col(\"session_month\") >= 3) & (col(\"session_month\") <= 5), \"Spring\")\n",
    "    .when((col(\"session_month\") >= 6) & (col(\"session_month\") <= 8), \"Summer\")\n",
    "    .when((col(\"session_month\") >= 9) & (col(\"session_month\") <= 11), \"Autumn\")\n",
    "    .otherwise(\"Winter\"),\n",
    ")\n",
    "\n",
    "#creating features in click_stream_df\n",
    "# defining categories based on the above\n",
    "biferations = {\n",
    "    \"Category 1\": [\"ADD_PROMO\", \"ADD_TO_CART\"],  # highly likely\n",
    "    \"Category 2\": [\"VIEW_PROMO\", \"VIEW_ITEM\", \"SEARCH\"],  # likey\n",
    "    \"Category 3\": [\"SCROLL\", \"HOMEPAGE\", \"CLICK\"],  # browsing\n",
    "}\n",
    "\n",
    "# creating a new column\n",
    "filtered_df = filtered_df.withColumn(\n",
    "    \"event_category\",\n",
    "    F.when(F.col(\"event_name\").isin(biferations[\"Category 1\"]), \"Category 1\")\n",
    "    .when(F.col(\"event_name\").isin(biferations[\"Category 2\"]), \"Category 2\")\n",
    "    .when(F.col(\"event_name\").isin(biferations[\"Category 3\"]), \"Category 3\")\n",
    "    .otherwise(\"Unknown\"),\n",
    ")\n",
    "\n",
    "# creating new columns based on the categories stated to aggregate actions based on session IDs\n",
    "filtered_df = filtered_df.groupBy(\"session_id\", \"customer_id\", \"is_promotion\", \"session_season\", \"event_time\", \"event_name\", \"event_metadata\").agg(\n",
    "    sum(when(col(\"event_category\") == \"Category 1\", 1).otherwise(0)).alias(\n",
    "        \"num_cat_highvalue\"\n",
    "    ),\n",
    "    sum(when(col(\"event_category\") == \"Category 2\", 1).otherwise(0)).alias(\n",
    "        \"num_cat_midvalue\"\n",
    "    ),\n",
    "    sum(when(col(\"event_category\") == \"Category 3\", 1).otherwise(0)).alias(\n",
    "        \"num_cat_lowvalue\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = filtered_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "#getting the features from transaction table to the live stream dataframe\n",
    "filtered_df = filtered_df.withColumn(\"shipment_fee_paid\", lit(0))\n",
    "\n",
    "filtered_df = filtered_df.withColumn(\"payment_method\", lit(\"Credit Card\"))\n",
    "\n",
    "filtered_df = filtered_df.withColumn(\"total_amount_quartile\", lit(\"Q3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = filtered_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining the filtered_df with customer_df\n",
    "features_df = filtered_df.join(customer_df, on = \"customer_id\", how = \"left\")\n",
    "#defining the columns in the features table\n",
    "features_df = features_df.select(\n",
    "    \"customer_id\",\n",
    "    \"num_cat_highvalue\",\n",
    "    \"num_cat_midvalue\",\n",
    "    \"num_cat_lowvalue\",\n",
    "    \"is_promotion\",\n",
    "    \"session_season\",\n",
    "    \"gender\",\n",
    "    \"age\",\n",
    "    \"device_type\",\n",
    "    \"customer_since\",\n",
    "    \"city_tier\",\n",
    "    \"shipment_fee_paid\",\n",
    "    \"payment_method\",\n",
    "    \"total_amount_quartile\",\n",
    "    \"event_name\",\n",
    "    \"event_metadata\",\n",
    "    \"event_time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = features_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Load your ML model, and use the model to predict if each session will purchase according to the requirements below:\n",
    "a) Every 10 seconds, show the total number of potential sales transactions (prediction = 1) in the last 1 minute.   \n",
    "b) Every 30 seconds, show the total potential revenue in the last 30 seconds. “Potiential revenue” here is definded as: When prediction=1, extract customer shopping cart detail from metadata (sum of all items of ADD_TO_CART events).  \n",
    "c) Every 1 minute, show the top 10 best-selling products by total quantity. (note: No historical data is required, only the top 10 in each 1 minute window.)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model\n",
    "from pyspark.ml import PipelineModel\n",
    "#loading the model\n",
    "model = PipelineModel.load(\"FinalModelA2A/GBT_Model\")\n",
    "#transforming the data\n",
    "prediction = model.transform(features_df)\n",
    "#selecting columns from the resulting df\n",
    "result_df = prediction.select(\n",
    "    \"customer_id\",\n",
    "    \"num_cat_highvalue\",\n",
    "    \"num_cat_midvalue\",\n",
    "    \"num_cat_lowvalue\",\n",
    "    \"is_promotion\",\n",
    "    \"session_season\",\n",
    "    \"gender\",\n",
    "    \"age\",\n",
    "    \"device_type\",\n",
    "    \"customer_since\",\n",
    "    \"city_tier\",\n",
    "    \"shipment_fee_paid\",\n",
    "    \"payment_method\",\n",
    "    \"total_amount_quartile\",\n",
    "    \"event_time\",\n",
    "    \"event_name\",\n",
    "    \"event_metadata\",\n",
    "    \"prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = result_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a\n",
    "from pyspark.sql.functions import *\n",
    "#predicting or total number of sales predicted in 1 minute triggered every 10 seconds\n",
    "window_prediction = result_df \\\n",
    "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
    "    .groupBy(window(\"event_time\", \"10 seconds\"), \"customer_id\") \\\n",
    "    .agg(sum(\"prediction\").alias(\"total\")) \\\n",
    "    .select(\"window\", \"customer_id\", \"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = window_prediction \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType, MapType, StringType, IntegerType\n",
    "#filtering for add to cart data only\n",
    "add_to_cart_df = result_df.filter(result_df.event_name == \"ADD_TO_CART\")\n",
    "#defining the schema\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"item_price\", IntegerType(), True),\n",
    "])\n",
    "#adding the metadata column\n",
    "add_to_cart_df = add_to_cart_df.withColumn(\"metadata\", from_json(col(\"event_metadata\"), metadata_schema))\n",
    "#calculating revenue\n",
    "add_to_cart_df = add_to_cart_df.withColumn(\"revenue\", col(\"metadata.quantity\") * col(\"metadata.item_price\"))\n",
    "#getting data as per the requested format\n",
    "windowed_data = add_to_cart_df.groupBy(window(\"event_time\", \"30 seconds\", \"30 seconds\")) \\\n",
    "    .agg(sum(\"revenue\").alias(\"potential_revenue\"))\\\n",
    "    .select(\"window\", \"potential_revenue\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = windowed_data \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime = \"30 seconds\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num_cat_highvalue: long (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- num_cat_lowvalue: long (nullable = true)\n",
      " |-- is_promotion: integer (nullable = false)\n",
      " |-- session_season: string (nullable = false)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- customer_since: integer (nullable = true)\n",
      " |-- city_tier: string (nullable = true)\n",
      " |-- shipment_fee_paid: integer (nullable = false)\n",
      " |-- payment_method: string (nullable = false)\n",
      " |-- total_amount_quartile: string (nullable = false)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- item_price: integer (nullable = true)\n",
      " |-- revenue: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selecting data needed for further calculations\n",
    "add_to_cart_df = add_to_cart_df.select(\n",
    "    \"num_cat_highvalue\",\n",
    "    \"num_cat_midvalue\",\n",
    "    \"num_cat_lowvalue\",\n",
    "    \"is_promotion\",\n",
    "    \"session_season\",\n",
    "    \"gender\",\n",
    "    \"age\",\n",
    "    \"device_type\",\n",
    "    \"customer_since\",\n",
    "    \"city_tier\",\n",
    "    \"shipment_fee_paid\",\n",
    "    \"payment_method\",\n",
    "    \"total_amount_quartile\",\n",
    "    \"event_time\",\n",
    "    \"event_name\",\n",
    "    \"event_metadata\",\n",
    "    \"prediction\",\n",
    "    \"metadata.product_id\",\n",
    "    \"metadata.quantity\",\n",
    "    \"metadata.item_price\",\n",
    "    \"revenue\"\n",
    ")\n",
    "\n",
    "add_to_cart_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c\n",
    "from pyspark.sql.functions import *\n",
    "#identifying the top products\n",
    "top_products_df = add_to_cart_df \\\n",
    "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
    "    .groupBy(window(\"event_time\", \"1 minute\"), \"product_id\") \\\n",
    "    .agg(sum(\"quantity\").alias(\"total_quantity\"))\n",
    "#defining the windoe\n",
    "top_10 = top_products_df\\\n",
    "    .orderBy(\"window\", col(\"total_quantity\").desc())\n",
    "#sorting\n",
    "top_10 = top_10.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = top_10 \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7  \n",
    "a) Persist the prediction result along with cart metadata in parquet format; after that, read the parquet file and show the results to verify it is saved properly.  \n",
    "b) Persist the 30-second sales prediction in another parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7a\n",
    "#securing data for writing the first parquet file\n",
    "parquet1_df = result_df.select(\n",
    "    \"event_time\",\n",
    "    \"prediction\",\n",
    "    \"event_metadata\",\n",
    "    \"customer_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a write stream for the first file\n",
    "windowed_df = parquet1_df.writeStream.format(\"parquet\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\", \"stream_parquet/parquet\")\\\n",
    "    .option(\"checkpointLocation\", \"stream_parquet/click_stream\")\\\n",
    "    .trigger(processingTime = \"5 seconds\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_df.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      "\n",
      "+-------------------+----------+--------------------+-----------+\n",
      "|         event_time|prediction|      event_metadata|customer_id|\n",
      "+-------------------+----------+--------------------+-----------+\n",
      "|2023-10-18 23:02:12|       0.0|               \"NaN\"|      75437|\n",
      "|2023-10-18 23:02:32|       1.0|               \"NaN\"|      93707|\n",
      "|2023-10-18 23:02:11|       1.0|               \"NaN\"|      62179|\n",
      "|2023-10-18 23:01:44|       1.0|               \"NaN\"|       6308|\n",
      "|2023-10-18 23:02:26|       1.0|{'product_id': 44...|       4037|\n",
      "|2023-10-18 23:01:46|       1.0|               \"NaN\"|      15252|\n",
      "|2023-10-18 23:02:20|       1.0|{'product_id': 10...|      70519|\n",
      "|2023-10-18 23:02:31|       0.0|               \"NaN\"|      30105|\n",
      "|2023-10-18 23:02:06|       1.0|{'product_id': 54...|      40436|\n",
      "|2023-10-18 23:02:16|       1.0|               \"NaN\"|      12353|\n",
      "|2023-10-18 23:02:30|       1.0|               \"NaN\"|      91560|\n",
      "|2023-10-18 23:01:44|       0.0|               \"NaN\"|      38133|\n",
      "|2023-10-18 23:01:41|       1.0|{'product_id': 43...|      19020|\n",
      "|2023-10-18 23:01:43|       1.0|               \"NaN\"|      61545|\n",
      "|2023-10-18 23:01:49|       0.0|               \"NaN\"|      97550|\n",
      "|2023-10-18 23:01:57|       0.0|{'product_id': 26...|      70112|\n",
      "|2023-10-18 23:01:43|       1.0|{'product_id': 56...|      20799|\n",
      "|2023-10-18 23:02:04|       1.0|               \"NaN\"|      14391|\n",
      "|2023-10-18 23:01:57|       1.0|{'product_id': 56...|      47572|\n",
      "|2023-10-18 23:02:09|       0.0|               \"NaN\"|      37636|\n",
      "+-------------------+----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#writing the first file using the schema as the reading data\n",
    "parquet1_read_df = spark.read.schema(parquet1_df.schema).parquet(\"stream_parquet/parquet\")\n",
    "parquet1_read_df.printSchema()\n",
    "parquet1_read_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b\n",
    "#defining data for the second parquet file\n",
    "parquet2_df = windowed_data.withColumn(\"start\", col(\"window.start\"))\\\n",
    "    .withColumn(\"end\", col(\"window.end\"))\\\n",
    "    .withColumn(\"sales\", col(\"potential_revenue\"))\\\n",
    "    .select(\"start\", \"end\", \"sales\")\n",
    "#wrting the second parquet file\n",
    "windowed_sales_df = parquet2_df.writeStream.format(\"parquet\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\", \"sales_parquet/revenue_parquet\")\\\n",
    "    .option(\"checkpointLocation\", \"sales_parquet/revenue\")\\\n",
    "    .trigger(processingTime = \"1 second\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_sales_df.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start: timestamp (nullable = true)\n",
      " |-- end: timestamp (nullable = true)\n",
      " |-- sales: long (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+---------+\n",
      "|              start|                end|    sales|\n",
      "+-------------------+-------------------+---------+\n",
      "|2023-10-18 23:00:00|2023-10-18 23:00:30|207151450|\n",
      "|2023-10-18 23:01:00|2023-10-18 23:01:30| 12587190|\n",
      "|2023-10-18 22:59:30|2023-10-18 23:00:00|143672580|\n",
      "|2023-10-18 23:01:30|2023-10-18 23:02:00|212926400|\n",
      "|2023-10-18 23:02:00|2023-10-18 23:02:30|240820842|\n",
      "|2023-10-18 23:02:30|2023-10-18 23:03:00| 10846872|\n",
      "|2023-10-18 23:03:30|2023-10-18 23:04:00|126389884|\n",
      "|2023-10-18 23:00:30|2023-10-18 23:01:00| 98021985|\n",
      "|2023-10-18 23:04:00|2023-10-18 23:04:30|206800774|\n",
      "|2023-10-18 23:04:30|2023-10-18 23:05:00|239272222|\n",
      "|2023-10-18 23:05:00|2023-10-18 23:05:30|219125678|\n",
      "+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading the second parquet file\n",
    "parquet2_read_df = spark.read.schema(parquet2_df.schema).parquet(\"sales_parquet/revenue_parquet\")\n",
    "parquet2_read_df.printSchema()\n",
    "parquet2_read_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8  \n",
    "Read the parquet files as a data stream, for 7a) join customer information and send to a Kafka topic with an appropriate name to the data visualisation. For 7b) Send the message directly to another Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka3 import KafkaProducer\n",
    "from json import dumps\n",
    "from time import sleep\n",
    "\n",
    "#configuation\n",
    "hostip = \"192.168.0.250\" #change the IP4 address as needed\n",
    "#defining the topic\n",
    "pred_topic = \"prediction_final\"\n",
    "sale_topic = \"sales_final\"\n",
    "#defining the broker\n",
    "bootstrap_servers = f'{hostip}:9092'\n",
    "\n",
    "# defining the Kafka producer instance\n",
    "# using value serializer to serialise the data efore sending it to a kafka topic\n",
    "def connect_kafka():\n",
    "    producer = None\n",
    "    try:\n",
    "        producer = KafkaProducer(bootstrap_servers=bootstrap_servers, value_serializer=lambda r: dumps(r).encode('utf-8'))\n",
    "        return producer\n",
    "    except Exception as e:\n",
    "        print(\"Exception while connecting to Kafka\")\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "def publish_message(producer_instance, topic_name, data):\n",
    "    try:\n",
    "        #sending the read data to the kafka topic\n",
    "        producer_instance.send(topic_name, data)\n",
    "        #successfully sent\n",
    "        print('Message published successfully. Data: ' + str(data))\n",
    "    except Exception as ex:\n",
    "        #handling exception\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Stream 1\n",
    "#defining schema for the parquet file\n",
    "parquet_1_schema = parquet1_df.schema\n",
    "# creating a stream for reading data from file\n",
    "prediction_stream_df = spark.readStream.schema(parquet_1_schema).parquet(\"stream_parquet/parquet\")\n",
    "#joining with customer_data\n",
    "prediction_final_df = prediction_stream_df.join(customer_df, \"customer_id\", \"left\")\n",
    "prediction_final_df = prediction_final_df.withColumn(\"event_time\", col(\"event_time\").cast(StringType()))\n",
    "prediction_final_df = prediction_final_df.withColumn(\"birthdate\", col(\"birthdate\").cast(StringType()))\n",
    "prediction_final_df = prediction_final_df.withColumn(\"first_join_date\", col(\"first_join_date\").cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = prediction_final_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to convert row to dict and send it to kafka\n",
    "def send_kafka(row):\n",
    "    data = row.asDict()\n",
    "    pred_producer = connect_kafka()\n",
    "    publish_message(pred_producer, pred_topic, data)\n",
    "    \n",
    "query = prediction_final_df.writeStream\\\n",
    "    .foreach(send_kafka)\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start: string (nullable = true)\n",
      " |-- end: string (nullable = true)\n",
      " |-- sales: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream 2\n",
    "#defining schema for the parquet file\n",
    "parquet_2_schema = parquet2_df.schema\n",
    "# creating a stream for reading data from file\n",
    "sales_stream_df = spark.readStream.schema(parquet_2_schema).parquet(\"sales_parquet/revenue_parquet\")\n",
    "\n",
    "\n",
    "sales_stream_df = sales_stream_df.withColumn(\"start\", col(\"start\").cast(StringType()))\n",
    "sales_stream_df = sales_stream_df.withColumn(\"end\", col(\"end\").cast(StringType()))\n",
    "\n",
    "sales_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to convert row to dict and send it to kafka\n",
    "def send_kafka(row):\n",
    "    data = row.asDict()\n",
    "    sale_producer = connect_kafka()\n",
    "    publish_message(sale_producer, sale_topic, data)\n",
    "    \n",
    "query = sales_stream_df.writeStream\\\n",
    "    .foreach(send_kafka)\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
